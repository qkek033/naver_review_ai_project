# app.py

import streamlit as st
import pandas as pd
import re
import time
import torch
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration
from transformers import ElectraTokenizer, ElectraForSequenceClassification

# ====================
# 1. ëª¨ë¸ ë¡œë“œ (ìš”ì•½ + ê°ì„±)
# ====================

@st.cache_resource
def load_models():
    kobart_tokenizer = PreTrainedTokenizerFast.from_pretrained("digit82/kobart-summarization")
    kobart_model = BartForConditionalGeneration.from_pretrained("digit82/kobart-summarization")

    koelectra_tokenizer = ElectraTokenizer.from_pretrained("monologg/koelectra-base-discriminator")
    koelectra_model = ElectraForSequenceClassification.from_pretrained(
        "monologg/koelectra-base-discriminator", num_labels=2)
    koelectra_model.eval()
    
    return kobart_tokenizer, kobart_model, koelectra_tokenizer, koelectra_model


# ====================
# 2. í…ìŠ¤íŠ¸ ì²˜ë¦¬ í•¨ìˆ˜
# ====================

def clean_text(text):
    text = re.sub(r'[^\uAC00-\uD7A3a-zA-Z0-9\s]', '', str(text))  # íŠ¹ìˆ˜ë¬¸ì ì œê±°
    text = re.sub(r'\s+', ' ', text).strip()
    return text


def kobart_summarize(text, tokenizer, model):
    input_ids = tokenizer.encode(text, return_tensors='pt', max_length=512, truncation=True)
    summary_ids = model.generate(input_ids, max_length=64, num_beams=4, early_stopping=True)
    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)


def predict_sentiment(text, tokenizer, model):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=256)
    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits
        predicted = torch.argmax(logits, dim=1).item()
    return "ê¸ì •" if predicted == 1 else "ë¶€ì •"


# ====================
# 3. ì›¹ í¬ë¡¤ë§ í•¨ìˆ˜
# ====================

def crawl_reviews(url, pages=5):
    chrome_path = "C:/tools/chromedriver-win64/chromedriver.exe"
    options = webdriver.ChromeOptions()
    options.add_argument("headless")
    service = Service(chrome_path)
    driver = webdriver.Chrome(service=service, options=options)
    driver.get(url)

    review_list = []
    for page in range(pages):
        time.sleep(2)
        try:
            review_items = WebDriverWait(driver, 10).until(
                EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'div._1kMfD5ErZ6 > span._2L3vDiadT9'))
            )
            for r in review_items:
                text = r.text.strip()
                if text:
                    review_list.append(text)
            print(f"{page + 1}í˜ì´ì§€ ìˆ˜ì§‘ ì™„ë£Œ")
        except:
            print(f"{page + 1}í˜ì´ì§€ ìˆ˜ì§‘ ì‹¤íŒ¨")
            break
        try:
            next_btn = driver.find_element(By.CSS_SELECTOR, 'a.fAUKmK')
            next_btn.click()
        except:
            break
    driver.quit()
    return review_list


# ====================
# 4. ë©”ì¸ Streamlit ì•±
# ====================

def main():
    st.set_page_config(page_title="ë¦¬ë·° ìš”ì•½ + ê°ì„±ë¶„ì„ ì•±", layout="wide")
    st.title("ğŸ“ ë„¤ì´ë²„ ë¦¬ë·° ìš”ì•½ & ê°ì„±ë¶„ì„")

    kobart_tokenizer, kobart_model, koelectra_tokenizer, koelectra_model = load_models()

    # 1ë‹¨ê³„: ë¦¬ë·° ìˆ˜ì§‘
    with st.expander("ğŸ“Œ Step 1. ë¦¬ë·° ìˆ˜ì§‘"):
        url = st.text_input("ë¦¬ë·° í˜ì´ì§€ URL ì…ë ¥")
        pages = st.slider("ìˆ˜ì§‘í•  í˜ì´ì§€ ìˆ˜", 1, 10, 3)
        if st.button("ë¦¬ë·° ìˆ˜ì§‘ ì‹œì‘"):
            with st.spinner("ë¦¬ë·° ìˆ˜ì§‘ ì¤‘..."):
                reviews = crawl_reviews(url, pages)
                df = pd.DataFrame(reviews, columns=['review'])
                df.to_csv("naver_reviews_raw.csv", index=False, encoding='utf-8-sig')
                st.success(f"âœ… ì´ {len(reviews)}ê°œ ë¦¬ë·° ìˆ˜ì§‘ ì™„ë£Œ!")

    # 2ë‹¨ê³„: ì „ì²˜ë¦¬
    with st.expander("ğŸ§¹ Step 2. ì „ì²˜ë¦¬"):
        try:
            df = pd.read_csv("naver_reviews_raw.csv")
            df['cleaned'] = df['review'].apply(clean_text)
            st.write(df[['review', 'cleaned']].head())
            df.to_csv("naver_reviews_cleaned.csv", index=False, encoding='utf-8-sig')
            st.success("âœ… ì „ì²˜ë¦¬ ì™„ë£Œ")
        except:
            st.warning("âš ï¸ ë¨¼ì € ë¦¬ë·°ë¥¼ ìˆ˜ì§‘í•´ì£¼ì„¸ìš”.")

    # 3ë‹¨ê³„: ìš”ì•½ ë° ê°ì„±ë¶„ì„
    with st.expander("ğŸ” Step 3. ìš”ì•½ & ê°ì„±ë¶„ì„"):
        try:
            df = pd.read_csv("naver_reviews_cleaned.csv")
            summaries = []
            sentiments = []
            progress = st.progress(0)
            for i, review in enumerate(df['cleaned']):
                try:
                    summary = kobart_summarize(review, kobart_tokenizer, kobart_model)
                    sentiment = predict_sentiment(review, koelectra_tokenizer, koelectra_model)
                except:
                    summary = "ìš”ì•½ ì‹¤íŒ¨"
                    sentiment = "ë¶„ì„ ì‹¤íŒ¨"
                summaries.append(summary)
                sentiments.append(sentiment)
                progress.progress((i+1)/len(df))
            df['summary'] = summaries
            df['sentiment'] = sentiments
            df.to_csv("naver_reviews_summary_sentiment.csv", index=False, encoding='utf-8-sig')
            st.dataframe(df[['cleaned', 'summary', 'sentiment']].head())
            st.success("âœ… ìš”ì•½ ë° ê°ì„±ë¶„ì„ ì™„ë£Œ")
        except:
            st.warning("âš ï¸ ì „ì²˜ë¦¬ëœ ë¦¬ë·°ë¥¼ ë¨¼ì € ì¤€ë¹„í•´ì£¼ì„¸ìš”.")

    # 4ë‹¨ê³„: ì‹œê°í™”
    with st.expander("ğŸ“Š Step 4. ê°ì„±ë¶„ì„ ì‹œê°í™”"):
        try:
            df = pd.read_csv("naver_reviews_summary_sentiment.csv")
            sentiment_counts = df['sentiment'].value_counts()

            col1, col2 = st.columns(2)
            with col1:
                st.subheader("ë§‰ëŒ€ê·¸ë˜í”„")
                fig, ax = plt.subplots()
                ax.bar(sentiment_counts.index, sentiment_counts.values, color=["green", "red"])
                ax.set_title("ê¸ì • vs ë¶€ì • ë¦¬ë·° ìˆ˜")
                st.pyplot(fig)

            with col2:
                st.subheader("íŒŒì´ì°¨íŠ¸")
                fig, ax = plt.subplots()
                ax.pie(sentiment_counts, labels=sentiment_counts.index, autopct="%1.1f%%", colors=["green", "red"])
                ax.set_title("ë¦¬ë·° ë¹„ìœ¨")
                st.pyplot(fig)

            # WordCloud
            st.subheader("ì›Œë“œí´ë¼ìš°ë“œ")
            pos_text = " ".join(df[df['sentiment'] == 'ê¸ì •']['cleaned'])
            neg_text = " ".join(df[df['sentiment'] == 'ë¶€ì •']['cleaned'])
            font_path = "C:/Windows/Fonts/malgun.ttf"

            wc = WordCloud(font_path=font_path, background_color='white', width=800, height=400)

            col1, col2 = st.columns(2)
            with col1:
                st.markdown("### ğŸ‘ ê¸ì • ë¦¬ë·° WordCloud")
                st.image(wc.generate(pos_text).to_array())
            with col2:
                st.markdown("### ğŸ‘ ë¶€ì • ë¦¬ë·° WordCloud")
                st.image(wc.generate(neg_text).to_array())
        except:
            st.warning("âš ï¸ ë¶„ì„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë¶„ì„ì„ ì™„ë£Œí•´ì£¼ì„¸ìš”.")

if __name__ == "__main__":
    main()
